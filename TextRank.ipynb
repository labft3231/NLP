{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "from newspaper import Article\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceTokenizer(object): \n",
    "    def __init__(self): \n",
    "        self.kkma = Kkma()\n",
    "        self.twitter = Twitter()\n",
    "        self.stopwords = ['중인','만큼','마찬가지','꼬집었',\"연합뉴스\",\"데일리\",\"동아일보\",\"중앙일보\",\"조선일보\",\"기자\",\"아\",\"휴\",\"아이구\",\"아이쿠\",\"아이고\",\"어\",\"나\",\"우리\",\"저희\",\"따라\",\"의해\",\"을\",\"를\",\"에\",\"의\",\"가\"]\n",
    "        \n",
    "        \n",
    "    def url2sentences(self, url): \n",
    "        article = Article(url, language = 'ko')\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        sentences = self.kkma.sentences(article.text)\n",
    "        for idx in range(0, len(sentences)) : \n",
    "            if len(sentences[idx]) <= 10 : \n",
    "                sentences[idx - 1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = '' \n",
    "        return sentences \n",
    "    \n",
    "    def text2sentences(self, text) : \n",
    "        sentences = self.kkma.sentences(text)\n",
    "        for idx in range(0, len(sentences)) : \n",
    "            if len(sentences[idx]) <= 10 : \n",
    "                sentences[idx - 1] += (' ' + sentences[idx])\n",
    "                sentences[idx] = '' \n",
    "        return sentences \n",
    "                        \n",
    "    def get_nouns(self, sentences) : \n",
    "        nouns = [] \n",
    "        for sentence in sentences : \n",
    "            if sentence is not '' : \n",
    "                nouns.append(' '.join([noun for noun in self.twitter.nouns(str(sentence)) if noun not in self.stopwords and len(noun) > 1]) )\n",
    "        return nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphMatrix(object):\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer()\n",
    "        self.cnt_vec = CountVectorizer()\n",
    "        self.graph_sentence = []\n",
    "        \n",
    "    def build_sent_graph(self, sentence):\n",
    "        tfidf_mat = self.tfidf.fit_transform(sentence).toarray()\n",
    "        self.graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "        return self.graph_sentence\n",
    "    def build_words_graph(self, sentence):\n",
    "        cnt_vec_mat = normalize(self.cnt_vec.fit_transform(sentence).toarray().astype(float), axis=0)\n",
    "        vocab = self.cnt_vec.vocabulary_\n",
    "        return np.dot(cnt_vec_mat.T, cnt_vec_mat), {vocab[word] : word for word in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rank(object):\n",
    "    def get_ranks(self, graph, d=0.85): # d = damping factor\n",
    "        A = graph\n",
    "        matrix_size = A.shape[0]\n",
    "        for id in range(matrix_size):\n",
    "            A[id, id] = 0 # diagonal 부분을 0으로\n",
    "            link_sum = np.sum(A[:,id]) # A[:, id] = A[:][id]\n",
    "\n",
    "            if link_sum != 0:\n",
    "                A[:, id] /= link_sum\n",
    "            A[:, id] *= -d\n",
    "            A[id, id] = 1\n",
    "        B = (1-d) * np.ones((matrix_size, 1))\n",
    "        ranks = np.linalg.solve(A, B) # 연립방정식 Ax = b\n",
    "        return {idx: r[0] for idx, r in enumerate(ranks)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank(object):\n",
    "    def __init__(self, text):\n",
    "        self.sent_tokenize = SentenceTokenizer()\n",
    "        if text[:5] in ('http:', 'https'):\n",
    "            self.sentences = self.sent_tokenize.url2sentences(text)\n",
    "        else:\n",
    "            self.sentences = self.sent_tokenize.text2sentences(text)\n",
    "        self.nouns = self.sent_tokenize.get_nouns(self.sentences)\n",
    "        self.graph_matrix = GraphMatrix()\n",
    "        self.sent_graph = self.graph_matrix.build_sent_graph(self.nouns)\n",
    "        self.words_graph, self.idx2word = self.graph_matrix.build_words_graph(self.nouns)\n",
    "        self.rank = Rank()\n",
    "        self.sent_rank_idx = self.rank.get_ranks(self.sent_graph)\n",
    "        self.sorted_sent_rank_idx = sorted(self.sent_rank_idx, key=lambda k: self.sent_rank_idx[k], reverse=True)\n",
    "        self.word_rank_idx = self.rank.get_ranks(self.words_graph)\n",
    "        self.sorted_word_rank_idx = sorted(self.word_rank_idx, key=lambda k: self.word_rank_idx[k], reverse=True)\n",
    "        \n",
    "        \n",
    "    def summarize(self, sent_num=3):\n",
    "        summary = []\n",
    "        index=[]\n",
    "        for idx in self.sorted_sent_rank_idx[:sent_num]:\n",
    "            index.append(idx)\n",
    "        index.sort()\n",
    "        for idx in index:\n",
    "            summary.append(self.sentences[idx])\n",
    "        return summary\n",
    "    def keywords(self, word_num=10):\n",
    "        rank = Rank()\n",
    "        rank_idx = rank.get_ranks(self.words_graph)\n",
    "        sorted_rank_idx = sorted(rank_idx, key=lambda k: rank_idx[k], reverse=True)\n",
    "        keywords = []\n",
    "        index=[]\n",
    "        for idx in sorted_rank_idx[:word_num]:\n",
    "            index.append(idx)\n",
    "        #index.sort()\n",
    "        for idx in index:\n",
    "            keywords.append(self.idx2word[idx])\n",
    "        return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ksc31\\desktop\\workspace\\ml_space\\venv\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소학교 때 책상을 같이 했던 아이들의 이름과, 패, 경, 옥 이런 이국 소녀들의 이름과 벌써 애기 어머니 된 계집애들의 이름과, 가난한 이웃사람들의 이름과, 비둘기, 강아지, 토끼, 노새, 노루, ｢ 프란시스․ 쟘｣ ｢ 라이 너․ 마리아․ 릴케｣ 이런 시인의 이름을 불러 봅니다.\n",
      "별이 아 슬히 멀 듯이, 어머님, 그리고 당신은 멀리 북간도에 계십니다.\n",
      "딴은 밤을 새워 우는 벌레는 부끄러운 이름을 슬퍼하는 까닭입니다.\n"
     ]
    }
   ],
   "source": [
    "url = 'http://v.media.daum.net/v/20170611192209012?rcmd=r'\n",
    "textrank = TextRank('''별 헤는 밤\n",
    "\n",
    "계절이 지나가는 하늘에는\n",
    "가을로 가득 차 있습니다.\n",
    "\n",
    "나는 아무 걱정도 없이\n",
    "가을 속의 별들을 다 헤일 듯합니다.\n",
    "\n",
    "가슴 속에 하나 둘 새겨지는 별을\n",
    "이제 다 못 헤는 것은\n",
    "쉬이 아침이 오는 까닭이요,\n",
    "내일 밤이 남은 까닭이요,\n",
    "아직 나의 청춘이 다하지 않은 까닭입니다.\n",
    "\n",
    "별 하나에 추억과\n",
    "별 하나에 사랑과\n",
    "별 하나에 쓸쓸함과\n",
    "별 하나에 동경과\n",
    "별 하나에 시와\n",
    "별 하나에 어머니, 어머니,\n",
    "\n",
    "어머님, 나는 별 하나에 아름다운 말 한마디씩 불러봅니다. 소학교때 책상을 같이 했던 아이들의 이름과, 패, 경, 옥 이런 이국소녀들의 이름과 벌써 애기 어머니 된 계집애들의 이름과, 가난한 이웃사람들의 이름과, 비둘기, 강아지, 토끼, 노새, 노루, ｢프란시스․쟘｣ ｢라이너․마리아․릴케｣ 이런 시인의 이름을 불러봅니다.\n",
    "\n",
    "이네들은 너무나 멀리 있습니다.\n",
    "별이 아슬히 멀 듯이,\n",
    "\n",
    "어머님,\n",
    "그리고 당신은 멀리 북간도에 계십니다.\n",
    "\n",
    "나는 무엇인지 그리워\n",
    "이 많은 별빛이 나린 언덕 위에\n",
    "내 이름자를 써보고,\n",
    "흙으로 덮어 버리었습니다.\n",
    "\n",
    "딴은 밤을 새워 우는 벌레는\n",
    "부끄러운 이름을 슬퍼하는 까닭입니다.\n",
    "\n",
    "그러나 겨울이 지나고 나의 별에도 봄이 오면\n",
    "무덤 위에 파란 잔디가 피어나듯이\n",
    "내 이름자 묻힌 언덕 위에도\n",
    "자랑처럼 풀이 무성할 게외다.\n",
    "''')\n",
    "for row in textrank.summarize(3):\n",
    "    print(row)\n",
    "#     print()\n",
    "#     print('keywords :',textrank.keywords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
